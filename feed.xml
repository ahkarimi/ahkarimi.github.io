<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ahkarimi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ahkarimi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-19T10:48:58+00:00</updated><id>https://ahkarimi.github.io/feed.xml</id><title type="html">blank</title><subtitle>My personal website. </subtitle><entry><title type="html">Removing Noise from ASR Text (Complete Code Walkthrough)</title><link href="https://ahkarimi.github.io/blog/2024/remove-noise-from-text/" rel="alternate" type="text/html" title="Removing Noise from ASR Text (Complete Code Walkthrough)"/><published>2024-11-01T15:09:00+00:00</published><updated>2024-11-01T15:09:00+00:00</updated><id>https://ahkarimi.github.io/blog/2024/remove-noise-from-text</id><content type="html" xml:base="https://ahkarimi.github.io/blog/2024/remove-noise-from-text/"><![CDATA[<div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024/cleaning_in_progress-480.webp 480w,/assets/img/blog/2024/cleaning_in_progress-800.webp 800w,/assets/img/blog/2024/cleaning_in_progress-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024/cleaning_in_progress.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image by Oliver Hale @ unsplash.com </div> <p>We’ve all been there. You’re trying to make sense of a conversation recorded during a phone call or transcribed from a voice assistant, and what you get is littered with noise: “uhh”, “em”, or endless repetitions of “aaa.” It’s frustrating to sift through the gibberish, and it makes automated text processing difficult. This challenge often arises when dealing with Automatic Speech Recognition (ASR) systems.</p> <p>Think about it—ASR systems are not perfect. They try their best to convert speech into text, but background noise, filler words, or even accents can introduce meaningless “noise” into the transcription. If you’re building an AI application to handle customer queries, or analyze voice calls, you need a way to filter out that junk, or your models might get confused by non-informative words.</p> <p>Now, imagine if we could automatically clean up such noisy transcriptions, removing meaningless noise while keeping the relevant text intact. Let’s walk through a solution for this problem, where we can filter out noisy tokens from a text by leveraging pre-trained transformers and masking.</p> <h3 id="the-solution-leveraging-masked-language-models">The Solution: Leveraging Masked Language Models</h3> <p>We’ll use a masked language model to help us identify which parts of the text are meaningful and which can be considered noise. The idea is to mask words one by one and have the model predict what should be in that position. If the model is highly confident that the masked word is correct (based on the context), we’ll keep it; otherwise, it’s likely to be noise and can be discarded.</p> <p>Here’s how we do it, step by step:</p> <ol> <li> <p><strong>Tokenizing the Text</strong>: We first break down the text into smaller units, called tokens. These tokens could be words, parts of words, or even single characters.</p> </li> <li> <p><strong>Masking One Token at a Time</strong>: For each token, we mask it—essentially hiding it from the model—and ask the model to predict what should be there. The model will give us a probability distribution of possible tokens for that masked position.</p> </li> <li> <p><strong>Filtering Based on Confidence</strong>: If the model’s confidence (or probability) for the original token is high, we assume the token is useful. If the confidence is low, it’s probably noise and we discard it.</p> </li> <li> <p><strong>Reconstructing the Text</strong>: Once we’ve filtered out the noisy tokens, we convert the valid tokens back into a clean string.</p> </li> </ol> <p>Let’s look at the code that implements this.</p> <p><br/></p> <h3 id="the-code">The Code</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">,</span> <span class="n">AutoModelForMaskedLM</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="n">model_name</span><span class="o">=</span><span class="sh">'</span><span class="s">google-bert/bert-base-uncased</span><span class="sh">'</span> <span class="c1"># you can use any MLM model from Huggingface
</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">token_ids</span><span class="p">])</span>

<span class="c1"># Mask each token one by one and get the model's prediction for the masked token
</span><span class="k">def</span> <span class="nf">filter_tokens_with_threshold</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="n">valid_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        
        <span class="n">masked_input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
        <span class="n">masked_input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token_id</span>  <span class="c1"># Mask the current token
</span>        
        <span class="c1"># Get model predictions for the masked token
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">masked_input_ids</span><span class="p">)</span>
        
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
        
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">token_prob</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">token_id</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
        
        <span class="c1"># Keep the token if its probability is above the threshold
</span>        <span class="k">if</span> <span class="n">token_prob</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="n">valid_tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">valid_tokens</span>

<span class="n">filtered_tokens</span> <span class="o">=</span> <span class="nf">filter_tokens_with_threshold</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">tokens</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">)</span>

<span class="n">filtered_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="n">filtered_tokens</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Filtered text: </span><span class="si">{</span><span class="n">filtered_text</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><br/> <br/></p> <h3 id="breaking-down-the-code">Breaking Down the Code</h3> <p><strong>1. Loading the Pre-trained Model and Tokenizer</strong> We begin by loading a pre-trained tokenizer and a masked language model (MLM) from the Hugging Face library. The <code class="language-plaintext highlighter-rouge">model_name</code> would be a model that suits your language needs (for example, <code class="language-plaintext highlighter-rouge">bert-base-uncased</code> for English).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForMaskedLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <p><strong>2. Tokenizing the Input Text</strong> Next, we break the input text into tokens. These tokens are then converted into token IDs, which is how the model understands the text.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <p><strong>3. Creating Input Tensors</strong> We convert the list of token IDs into a tensor, which is the format expected by the PyTorch-based model.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">token_ids</span><span class="p">])</span>
</code></pre></div></div> <p><br/></p> <p><strong>4. Masking Tokens and Predicting</strong> Here’s where the magic happens. For each token, we temporarily mask it and ask the model to predict what should be in its place. The model’s output gives us probabilities for every possible token. If the model’s prediction for the original token is above a set threshold, we keep the token; otherwise, it’s discarded as noise.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">masked_input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">mask_token_id</span>  <span class="c1"># Mask the current token
</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">masked_input_ids</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">.</span><span class="n">logits</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">token_prob</span> <span class="o">=</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">token_id</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>

<span class="k">if</span> <span class="n">token_prob</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
    <span class="n">valid_tokens</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div> <p><br/></p> <p><strong>5. Filtering with a Threshold</strong> We define a threshold to decide which tokens to keep. This threshold can be fine-tuned depending on how strict we want the filtering to be. Lower thresholds may retain more tokens, while higher thresholds may strip away more noise.</p> <p><strong>6. Reconstructing the Clean Text</strong> Finally, we take the remaining valid tokens and convert them back into a readable string format.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">filtered_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">convert_tokens_to_string</span><span class="p">(</span><span class="n">filtered_tokens</span><span class="p">)</span>
</code></pre></div></div> <p><br/> <br/></p> <h3 id="why-this-works">Why This Works</h3> <p>Masked language models, like BERT, are trained to predict missing words based on the context around them. When we mask a token in our noisy text and ask the model to predict the token that should be there, the model uses the surrounding context to make a highly informed guess. If it confidently predicts that a token should be in its place, we trust that the token is meaningful. If not, it’s probably just noise, and we can safely remove it.</p> <h3 id="the-result-cleaner-text">The Result: Cleaner Text</h3> <p>Using this method, we can filter out non-informative parts of a transcription, improving the quality of the text data. This is especially useful for downstream tasks like sentiment analysis, intent detection, or even summarization, where cleaner input can lead to more accurate outputs.</p> <p>For example, if our noisy input is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Hi, uh, I aaa want to, um, order a pizza."
</code></pre></div></div> <p>The filtered output might be:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Hi, I want to order a pizza."
</code></pre></div></div> <p>This approach can be extended and refined with larger datasets, adjusted thresholds, or custom fine-tuned models to adapt to specific domains like customer service or healthcare.</p> <p><br/></p> <h3 id="conclusion">Conclusion</h3> <p>As voice-based applications grow, dealing with noisy text is more important than ever. By using masked language models, we can filter out noise and improve the quality of transcriptions for better AI performance.</p> <p>I’m Amir Hossein Karimi, an NLP Engineer passionate about helping machines understand humans. If you found this useful, check out my other blogs for more on AI and natural language processing!</p>]]></content><author><name></name></author><category term="NLP"/><category term="noise_filtering"/><category term="text_cleaning"/><summary type="html"><![CDATA[This guide shows how to filter meaningless noise like 'um' and 'aaa' from text, improving quality for NLP tasks.]]></summary></entry></feed>